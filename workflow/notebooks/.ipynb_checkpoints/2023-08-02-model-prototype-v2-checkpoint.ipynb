{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Version 2\n",
    "\n",
    "Pick-up...\n",
    "- Try different loss functions\n",
    "    - Binomial\n",
    "    - Beta-binomial\n",
    "    - Probability itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 11:14:22.534412: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# import tensorflow.compat.v2 as tf\n",
    "# tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "import time\n",
    "from tensorflow_probability.python.layers.distribution_layer import DistributionLambda\n",
    "from tensorflow_probability.python import layers as tfpl\n",
    "from tensorflow_probability.python import distributions as tfd\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chr = \"chr17\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading/Writing\n",
    "\n",
    "We will make the intervals on the fly. We can use the `start` and `end` values for a given methylated position to index (via `iloc`) the corresponding encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_methylation_intervals(ifile):\n",
    "    # ifile = \"../../data/bed-intervals/253.chr17.bed\"\n",
    "    methy_data = pd.read_table(\n",
    "        ifile,\n",
    "        names = ['chrom', 'start', 'end', 'strand', 'methylated', 'total']\n",
    "    )    \n",
    "    return(methy_data)\n",
    "\n",
    "\n",
    "def read_encoded_data(ifile):\n",
    "    # ifile = \"../../data/encodings/253.chr17.001.bed\"\n",
    "    encoded_data = pd.read_table(\n",
    "        ifile, \n",
    "        names = ['chrom','start', 'end', 'A', 'C', 'G', 'T'],\n",
    "    )\n",
    "    \n",
    "    return(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>strand</th>\n",
       "      <th>methylated</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chr17</td>\n",
       "      <td>72177</td>\n",
       "      <td>73177</td>\n",
       "      <td>+/-</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chr17</td>\n",
       "      <td>72220</td>\n",
       "      <td>73220</td>\n",
       "      <td>+/-</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>chr17</td>\n",
       "      <td>72334</td>\n",
       "      <td>73334</td>\n",
       "      <td>+/-</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>chr17</td>\n",
       "      <td>72367</td>\n",
       "      <td>73367</td>\n",
       "      <td>+/-</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>chr17</td>\n",
       "      <td>72429</td>\n",
       "      <td>73429</td>\n",
       "      <td>+/-</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093484</th>\n",
       "      <td>1127406</td>\n",
       "      <td>chr17_KI270730v1_random</td>\n",
       "      <td>111677</td>\n",
       "      <td>112551</td>\n",
       "      <td>+/-</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093485</th>\n",
       "      <td>1127407</td>\n",
       "      <td>chr17_KI270730v1_random</td>\n",
       "      <td>111699</td>\n",
       "      <td>112551</td>\n",
       "      <td>+/-</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093486</th>\n",
       "      <td>1127408</td>\n",
       "      <td>chr17_KI270730v1_random</td>\n",
       "      <td>111737</td>\n",
       "      <td>112551</td>\n",
       "      <td>+/-</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093487</th>\n",
       "      <td>1127409</td>\n",
       "      <td>chr17_KI270730v1_random</td>\n",
       "      <td>111768</td>\n",
       "      <td>112551</td>\n",
       "      <td>+/-</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093488</th>\n",
       "      <td>1127410</td>\n",
       "      <td>chr17_KI270730v1_random</td>\n",
       "      <td>111795</td>\n",
       "      <td>112551</td>\n",
       "      <td>+/-</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1093489 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index                    chrom   start     end strand  methylated  \\\n",
       "0              0                    chr17   72177   73177    +/-          20   \n",
       "1              1                    chr17   72220   73220    +/-          15   \n",
       "2              5                    chr17   72334   73334    +/-          11   \n",
       "3              6                    chr17   72367   73367    +/-          10   \n",
       "4              7                    chr17   72429   73429    +/-          20   \n",
       "...          ...                      ...     ...     ...    ...         ...   \n",
       "1093484  1127406  chr17_KI270730v1_random  111677  112551    +/-          12   \n",
       "1093485  1127407  chr17_KI270730v1_random  111699  112551    +/-          12   \n",
       "1093486  1127408  chr17_KI270730v1_random  111737  112551    +/-          15   \n",
       "1093487  1127409  chr17_KI270730v1_random  111768  112551    +/-          19   \n",
       "1093488  1127410  chr17_KI270730v1_random  111795  112551    +/-          22   \n",
       "\n",
       "         total  \n",
       "0           20  \n",
       "1           16  \n",
       "2           11  \n",
       "3           11  \n",
       "4           20  \n",
       "...        ...  \n",
       "1093484     22  \n",
       "1093485     23  \n",
       "1093486     22  \n",
       "1093487     19  \n",
       "1093488     25  \n",
       "\n",
       "[1093489 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methy_data17 = read_methylation_intervals(\"../../data/bed-intervals/253.chr17.bed\")\n",
    "methy_data17 = methy_data17[methy_data17['total'] >=5 ].reset_index()\n",
    "encoding_data17 = read_encoded_data(\"../../data/encodings/253.chr17.bed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "methy_data18 = read_methylation_intervals(\"../../data/bed-intervals/253.chr18.bed\")\n",
    "methy_data18 = methy_data18[methy_data18['total'] >=5 ].reset_index()\n",
    "encoding_data18 = read_encoded_data(\"../../data/encodings/253.chr18.bed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "- 3 convolutional layers, ReLU activation\n",
    "    - First layer should give PWM\n",
    "    - Other two should continue to compress\n",
    "    - \n",
    "- 2 Fully-connected (dense) layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(methy_data, encodings, N, batch_size=8):\n",
    "    \n",
    "    # Initialize data\n",
    "    features = np.zeros(shape = (N, 1000, 4, 1), dtype=np.float32)\n",
    "    n_trials = np.zeros(shape = (N,), dtype=np.float32)\n",
    "    responses = np.zeros(shape = (N,), dtype=np.float32)\n",
    "    \n",
    "\n",
    "    for i, row in methy_data.iterrows():\n",
    "        # Grab one interval\n",
    "        if i >= N:\n",
    "            break\n",
    "\n",
    "        # And the encodings\n",
    "        zz = encodings.iloc[row['start']:row['end']][['A', 'C', 'G', 'T']].to_numpy()\n",
    "        features[i, :, :, :] = zz.reshape((1000, 4, 1))\n",
    "\n",
    "        # We can pull out the responses pretty easily\n",
    "        responses[i], n_trials[i] = (row['methylated'], row['total'])\n",
    "        \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, n_trials, responses))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 1000, 4, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = generate_dataset(methy_data17, encoding_data17, N=240000, batch_size=batch_size)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betabinomial_layer(x):\n",
    "    \n",
    "    # How many \n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    alpha, beta, p = tf.unstack(x, num=3, axis=-1)\n",
    "    \n",
    "    # Sigmoid function to map to (0,1)\n",
    "    alpha = tf.keras.activations.softmax(tf.expand_dims(alpha, - 1))\n",
    "    beta = tf.keras.activations.softmax(tf.expand_dims(beta, - 1))\n",
    "    p = tf.keras.activations.sigmoid(tf.expand_dims(p, - 1))\n",
    "    \n",
    "    out = tf.concat((alpha, beta, p), axis=num_dims - 1)\n",
    "    return(out)\n",
    "    \n",
    "    \n",
    "def vanilla_binomial_layer(x):\n",
    "    \n",
    "    # How many \n",
    "    num_dims = len(x.get_shape())\n",
    "    \n",
    "    p_degen, pi = tf.unstack(x, num=2, axis=-1)\n",
    "    \n",
    "    # Sigmoid function to map to (0,1)\n",
    "    p_degen = tf.keras.activations.sigmoid(tf.expand_dims(p_degen, - 1))\n",
    "    pi = tf.keras.activations.sigmoid(tf.expand_dims(pi, - 1))\n",
    "    \n",
    "    out = tf.concat((p0, pi), axis=num_dims - 1)\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def betabinomial_mean(estimated_params, trials):\n",
    "\n",
    "#     # Separate the parameters\n",
    "#     alpha, beta = tf.unstack(estimated_params, num=2, axis=-1)\n",
    "    \n",
    "#     # Add one dimension to make the right shape\n",
    "#     alpha = tf.expand_dims(alpha, -1)\n",
    "#     beta = tf.expand_dims(beta, -1)\n",
    "    \n",
    "    \n",
    "#     betabinomial_dist = tfp.distributions.BetaBinomial(\n",
    "#         total_count = trials, \n",
    "#         concentration1 = alpha, \n",
    "#         concentration0 = beta\n",
    "#     )\n",
    "    \n",
    "#     return(betabinomial_dist.mean())\n",
    "\n",
    "def negative_zinf_betabinomial_loss(estimated_params, trials, successes):\n",
    "    \n",
    "    # Separate the parameters\n",
    "    # These are [batch_size] \n",
    "    alpha, beta, p = tf.unstack(estimated_params, num=3, axis=-1)\n",
    "    pp = tf.stack([p, 1. - p], axis = 1)\n",
    "    \n",
    "    # Add one dimension to make the right shape\n",
    "    # These are [batch_size, 1]\n",
    "#     alpha = tf.expand_dims(alpha, -1)\n",
    "#     beta = tf.expand_dims(beta, -1)\n",
    "    \n",
    "    # Probabilities for routing compound\n",
    "    cat_dist = tfd.Categorical(probs = pp)\n",
    "    \n",
    "    bb_dist = tfd.BetaBinomial(\n",
    "        total_count = trials, \n",
    "        concentration1 = alpha, \n",
    "        concentration0 = beta\n",
    "    )\n",
    "        \n",
    "    zinf_bb_dist = tfd.Mixture(\n",
    "        # Routes the traffic\n",
    "        cat = cat_dist,\n",
    "        components = [tfd.Deterministic(tf.zeros_like(alpha)), bb_dist],\n",
    "    )\n",
    "    \n",
    "    ll = zinf_bb_dist.log_prob(successes)\n",
    "    return(-tf.reduce_mean(ll))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_zinf_betabinomial_loss(estimated_params, trials, successes):\n",
    "    \n",
    "    # Separate the parameters\n",
    "    # These are [batch_size] \n",
    "    p0, pi = tf.unstack(estimated_params, num=2, axis=-1)\n",
    "    pp = tf.stack([p0, 1. - p0], axis = 1)\n",
    "    \n",
    "    # Probabilities for routing compound\n",
    "    cat_dist = tfd.Categorical(probs = pp)\n",
    "    \n",
    "    bb_dist = tfd.Binomial(\n",
    "        total_count = trials, \n",
    "        probs = pi\n",
    "    )\n",
    "        \n",
    "    zinf_bb_dist = tfd.Mixture(\n",
    "        # Routes the traffic\n",
    "        cat = cat_dist,\n",
    "        components = [tfd.Deterministic(tf.zeros_like(alpha)), bb_dist],\n",
    "    )\n",
    "    \n",
    "    ll = zinf_bb_dist.log_prob(successes)\n",
    "    return(-tf.reduce_mean(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First 4 rows are encoding\n",
    "# Need the 1 to specify one-channel\n",
    "feature_inputs = keras.Input(shape=(1000, 4, 1))\n",
    "n_trials_inputs = keras.Input(shape = (1))\n",
    "\n",
    "\n",
    "# First convolutional layer doesn't compress too much\n",
    "# Filters are typically used for color channels, so we only need 1\n",
    "\n",
    "# Passing = same gives zero padding on edges\n",
    "x = keras.layers.Conv2D(filters = 1, \n",
    "                        kernel_size=(4,4), \n",
    "                        padding = \"same\", \n",
    "                        strides = 1)(feature_inputs)\n",
    "\n",
    "# Second does a more aggressive convolution\n",
    "x = keras.layers.Conv2D(filters = 1, kernel_size=(4,4), padding = \"same\", strides = (2, 2))(x)\n",
    "\n",
    "# x = keras.layers.Conv2D(filters = 1, kernel_size=(4,4), padding = \"same\", strides = (2, 2))(x)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "\n",
    "# Should still be shrinking\n",
    "x = keras.layers.Dense(8)(x)\n",
    "x = keras.layers.Dense(8)(x)\n",
    "\n",
    "x = keras.layers.Dense(3)(x)\n",
    "dist_outputs = tf.keras.layers.Lambda(prepare_for_rv_layer, name = \"map_to_bb_params\")(x)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=feature_inputs, outputs=dist_outputs, name=\"prototype\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"prototype\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 1000, 4, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 1000, 4, 1)        17        \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 500, 2, 1)         17        \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 8)                 8008      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 27        \n",
      "                                                                 \n",
      " map_to_bb_params (Lambda)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,141\n",
      "Trainable params: 8,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure looks right. Now we'll make sure that we can pass the data through..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = negative_zinf_betabinomial_loss,\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[negative_zinf_betabinomial_loss],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "We'll start by training on a small subset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensions\n",
    "\n",
    "We have a min-batch of 8 observations, each with 1000 \"rows\" and 4 \"columns\", and we have ~3k of these mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mean_abs_diff = keras.metrics.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(features, n_trials, n_methy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        params = model(features, training=True)\n",
    "        \n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = negative_zinf_betabinomial_loss(params, n_trials, n_methy)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    # Predicted (mode)\n",
    "#     n_methy_pred = betabinomial_mean(params, n_trials)\n",
    "    \n",
    "#     train_mean_abs_diff.update_state(n_methy, n_methy_pred)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "(8, 3)\n",
      "(8, 3)\n",
      "Training loss (for one batch) at step 0: 3.9585\n",
      "Seen so far: 8 samples\n",
      "Training loss (for one batch) at step 10000: 3.6979\n",
      "Seen so far: 80008 samples\n",
      "Training loss (for one batch) at step 20000: 3.5915\n",
      "Seen so far: 160008 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 3.4191\n",
      "Seen so far: 8 samples\n",
      "Training loss (for one batch) at step 10000: 3.5599\n",
      "Seen so far: 80008 samples\n",
      "Training loss (for one batch) at step 20000: 3.5995\n",
      "Seen so far: 160008 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (features, n_trials, n_methy) in enumerate(train_dataset):\n",
    "        \n",
    "        loss_value = train_step(features, n_trials, n_methy)\n",
    "\n",
    "\n",
    "        if step % 10000 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
